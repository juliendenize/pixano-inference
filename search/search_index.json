{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Pixano Inference Documentation!","text":"<p>Pixano Inference is a library of inference models for Pixano.</p> <p>Get started</p> <p>Check out the API reference</p> <p>Check out the documentation for Pixano</p>"},{"location":"code/","title":"Pixano Inference API reference","text":"<p>Here you will find the documentation for all of our implemented models.</p> <ul> <li>The pytorch_models module contains models from the PyTorch library.</li> <li>The segment_anything module contains the Segment Anything Model (SAM) for Meta.</li> <li>The tensorflow_models module contains models from the TensorFlow library.</li> </ul>"},{"location":"code/pytorch_models/deeplabv3/","title":"DeepLabV3","text":""},{"location":"code/pytorch_models/deeplabv3/#pytorch_models.DeepLabV3","title":"<code>pytorch_models.DeepLabV3(id='', device='cuda')</code>","text":"<p>         Bases: <code>InferenceModel</code></p> <p>PyTorch Hub DeepLabV3 Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>source</code> <code>str</code> <p>Model source</p> <code>info</code> <code>str</code> <p>Additional model info</p> <code>model</code> <code>torch.nn.Module</code> <p>PyTorch model</p> <code>transforms</code> <code>torch.nn.Module</code> <p>PyTorch preprocessing transforms</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/pytorch_models/deeplabv3.py</code> <pre><code>def __init__(self, id: str = \"\", device: str = \"cuda\") -&gt; None:\n\"\"\"Initialize model\n\n    Args:\n        id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    super().__init__(\n        name=\"DeepLabV3\",\n        id=id,\n        device=device,\n        source=\"PyTorch Hub\",\n        info=\"DeepLabV3, ResNet-50 Backbone\",\n    )\n\n    # Model\n    self.model = torch.hub.load(\n        \"pytorch/vision:v0.10.0\", \"deeplabv3_resnet50\", pretrained=True\n    )\n    self.model.eval()\n    self.model.to(self.device)\n\n    # Transforms\n    self.transforms = T.Compose(\n        [\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n    )\n</code></pre>"},{"location":"code/pytorch_models/deeplabv3/#pytorch_models.deeplabv3.DeepLabV3.inference_batch","title":"<code>inference_batch(batch, view, uri_prefix, threshold=0.0)</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[list[arrow_types.ObjectAnnotation]]</code> <p>list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation</p> Source code in <code>pixano_inference/pytorch_models/deeplabv3.py</code> <pre><code>def inference_batch(\n    self, batch: pa.RecordBatch, view: str, uri_prefix: str, threshold: float = 0.0\n) -&gt; list[list[arrow_types.ObjectAnnotation]]:\n\"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n\n    Returns:\n        list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation\n    \"\"\"\n\n    objects = []\n\n    # PyTorch Transforms don't support different-sized image batches, so iterate manually\n    for x in range(batch.num_rows):\n        # Preprocess image\n        im = batch[view][x].as_py(uri_prefix).as_pillow()\n        im_tensor = self.transforms(im).unsqueeze(0).to(self.device)\n\n        # Inference\n        with torch.no_grad():\n            output = self.model(im_tensor)[\"out\"][0]\n\n        # Process model outputs\n        sem_mask = output.argmax(0)\n        labels = torch.unique(sem_mask)[1:]\n        masks = sem_mask == labels[:, None, None]\n\n        objects.append(\n            [\n                arrow_types.ObjectAnnotation(\n                    id=shortuuid.uuid(),\n                    view_id=view,\n                    mask=mask_to_rle(unmold_mask(mask)),\n                    mask_source=self.id,\n                    category_id=int(label),\n                    category_name=voc_names(label),\n                )\n                for label, mask in zip(labels, masks)\n            ]\n        )\n    return objects\n</code></pre>"},{"location":"code/pytorch_models/maskrcnnv2/","title":"MaskRCNNv2","text":""},{"location":"code/pytorch_models/maskrcnnv2/#pytorch_models.MaskRCNNv2","title":"<code>pytorch_models.MaskRCNNv2(id='', device='cuda')</code>","text":"<p>         Bases: <code>InferenceModel</code></p> <p>PyTorch Hub MaskRCNNv2 Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>source</code> <code>str</code> <p>Model source</p> <code>info</code> <code>str</code> <p>Additional model info</p> <code>model</code> <code>torch.nn.Module</code> <p>PyTorch model</p> <code>transforms</code> <code>torch.nn.Module</code> <p>PyTorch preprocessing transforms</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/pytorch_models/maskrcnnv2.py</code> <pre><code>def __init__(self, id: str = \"\", device: str = \"cuda\") -&gt; None:\n\"\"\"Initialize model\n\n    Args:\n        id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    super().__init__(\n        name=\"MaskRCNNv2\",\n        id=id,\n        device=device,\n        source=\"PyTorch Hub\",\n        info=\"MaskRCNN, ResNet-50-FPN v2 Backbone, COCO_V1 Weights\",\n    )\n\n    # Model\n    self.model = maskrcnn_resnet50_fpn_v2(\n        weights=MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n    )\n    self.model.eval()\n    self.model.to(self.device)\n\n    # Transforms\n    self.transforms = MaskRCNN_ResNet50_FPN_V2_Weights.COCO_V1.transforms()\n</code></pre>"},{"location":"code/pytorch_models/maskrcnnv2/#pytorch_models.maskrcnnv2.MaskRCNNv2.inference_batch","title":"<code>inference_batch(batch, view, uri_prefix, threshold=0.0)</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[list[arrow_types.ObjectAnnotation]]</code> <p>list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation</p> Source code in <code>pixano_inference/pytorch_models/maskrcnnv2.py</code> <pre><code>def inference_batch(\n    self, batch: pa.RecordBatch, view: str, uri_prefix: str, threshold: float = 0.0\n) -&gt; list[list[arrow_types.ObjectAnnotation]]:\n\"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n\n    Returns:\n        list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation\n    \"\"\"\n\n    objects = []\n\n    # PyTorch Transforms don't support different-sized image batches, so iterate manually\n    for x in range(batch.num_rows):\n        # Preprocess image\n        im = batch[view][x].as_py(uri_prefix).as_pillow()\n        im_tensor = self.transforms(im).unsqueeze(0).to(self.device)\n\n        # Inference\n        with torch.no_grad():\n            output = self.model(im_tensor)[0]\n\n        # Process model outputs\n        w, h = im.size\n        objects.append(\n            [\n                arrow_types.ObjectAnnotation(\n                    id=shortuuid.uuid(),\n                    view_id=view,\n                    bbox=normalize(xyxy_to_xywh(output[\"boxes\"][i]), h, w),\n                    bbox_confidence=float(output[\"scores\"][i]),\n                    bbox_source=self.id,\n                    mask=mask_to_rle(unmold_mask(output[\"masks\"][i])),\n                    mask_source=self.id,\n                    category_id=int(output[\"labels\"][i]),\n                    category_name=coco_names_91(output[\"labels\"][i]),\n                )\n                for i in range(len(output[\"scores\"]))\n                if output[\"scores\"][i] &gt; threshold\n            ]\n        )\n    return objects\n</code></pre>"},{"location":"code/pytorch_models/yolov5/","title":"YOLOv5","text":""},{"location":"code/pytorch_models/yolov5/#pytorch_models.YOLOv5","title":"<code>pytorch_models.YOLOv5(size='s', id='', device='cuda')</code>","text":"<p>         Bases: <code>InferenceModel</code></p> <p>PyTorch Hub YOLOv5 Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>source</code> <code>str</code> <p>Model source</p> <code>info</code> <code>str</code> <p>Additional model info</p> <code>model</code> <code>torch.nn.Module</code> <p>PyTorch model</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>str</code> <p>Model size (\"n\", \"s\", \"m\", \"x\"). Defaults to \"s\".</p> <code>'s'</code> <code>id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/pytorch_models/yolov5.py</code> <pre><code>def __init__(self, size: str = \"s\", id: str = \"\", device: str = \"cuda\") -&gt; None:\n\"\"\"Initialize model\n\n    Args:\n        size (str, optional): Model size (\"n\", \"s\", \"m\", \"x\"). Defaults to \"s\".\n        id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    super().__init__(\n        name=f\"YOLOv5{size}\",\n        id=id,\n        device=device,\n        source=\"PyTorch Hub\",\n        info=f\"YOLOv5 model, {size.upper()} backbone\",\n    )\n\n    # Model\n    self.model = torch.hub.load(\n        \"ultralytics/yolov5\",\n        model=f\"yolov5{size}\",\n        pretrained=True,\n    )\n    self.model.to(self.device)\n</code></pre>"},{"location":"code/pytorch_models/yolov5/#pytorch_models.yolov5.YOLOv5.inference_batch","title":"<code>inference_batch(batch, view, uri_prefix, threshold=0.0)</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[list[arrow_types.ObjectAnnotation]]</code> <p>list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation</p> Source code in <code>pixano_inference/pytorch_models/yolov5.py</code> <pre><code>def inference_batch(\n    self, batch: pa.RecordBatch, view: str, uri_prefix: str, threshold: float = 0.0\n) -&gt; list[list[arrow_types.ObjectAnnotation]]:\n\"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n\n    Returns:\n        list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation\n    \"\"\"\n\n    # Preprocess image batch\n    im_batch = [\n        batch[view][x].as_py(uri_prefix).as_pillow() for x in range(batch.num_rows)\n    ]\n\n    # Inference\n    outputs = self.model(im_batch)\n\n    # Process model outputs\n    objects = []\n    for img, img_output in zip(im_batch, outputs.xyxy):\n        w, h = img.size\n        objects.append(\n            [\n                arrow_types.ObjectAnnotation(\n                    id=shortuuid.uuid(),\n                    view_id=view,\n                    bbox=normalize(xyxy_to_xywh(pred[0:4]), h, w),\n                    bbox_confidence=float(pred[4]),\n                    bbox_source=self.id,\n                    category_id=coco_ids_80to91(pred[5] + 1),\n                    category_name=coco_names_91(coco_ids_80to91(pred[5] + 1)),\n                )\n                for pred in img_output\n                if pred[4] &gt; threshold\n            ]\n        )\n\n    return objects\n</code></pre>"},{"location":"code/segment_anything/segment_anything/","title":"SAM","text":""},{"location":"code/segment_anything/segment_anything/#segment_anything.SAM","title":"<code>segment_anything.SAM(checkpoint_path, size='h', id='', device='cuda')</code>","text":"<p>         Bases: <code>InferenceModel</code></p> <p>Segment Anything Model (SAM)</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\")</p> <code>source</code> <code>str</code> <p>Model source</p> <code>info</code> <code>str</code> <p>Additional model info</p> <code>model</code> <code>torch.nn.Module</code> <p>SAM model</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>Path</code> <p>Model checkpoint path.</p> required <code>size</code> <code>str</code> <p>Model size (\"b\", \"l\", \"h\"). Defaults to \"h\".</p> <code>'h'</code> <code>id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".</p> <code>'cuda'</code> Source code in <code>pixano_inference/segment_anything/segment_anything.py</code> <pre><code>def __init__(\n    self,\n    checkpoint_path: Path,\n    size: str = \"h\",\n    id: str = \"\",\n    device: str = \"cuda\",\n) -&gt; None:\n\"\"\"Initialize model\n\n    Args:\n        checkpoint_path (Path): Model checkpoint path.\n        size (str, optional): Model size (\"b\", \"l\", \"h\"). Defaults to \"h\".\n        id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"cuda\", \"cpu\"). Defaults to \"cuda\".\n    \"\"\"\n\n    super().__init__(\n        name=f\"SAM_ViT_{size.upper()}\",\n        id=id,\n        device=device,\n        source=\"GitHub\",\n        info=f\"Segment Anything Model (SAM), ViT-{size.upper()} Backbone\",\n    )\n\n    # Model\n    self.model = sam_model_registry[f\"vit_{size}\"](checkpoint=checkpoint_path)\n    self.model.to(device=self.device)\n\n    # Model path\n    self.checkpoint_path = checkpoint_path\n</code></pre>"},{"location":"code/segment_anything/segment_anything/#segment_anything.segment_anything.SAM.embedding_batch","title":"<code>embedding_batch(batch, view, uri_prefix)</code>","text":"<p>Embedding precomputing for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <p>Returns:</p> Type Description <code>list[np.ndarray]</code> <p>list[np.ndarray]: Model embeddings as NumPy arrays</p> Source code in <code>pixano_inference/segment_anything/segment_anything.py</code> <pre><code>def embedding_batch(\n    self, batch: pa.RecordBatch, view: str, uri_prefix: str\n) -&gt; list[np.ndarray]:\n\"\"\"Embedding precomputing for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n\n    Returns:\n        list[np.ndarray]: Model embeddings as NumPy arrays\n    \"\"\"\n\n    embeddings = []\n\n    # Iterate manually\n    for x in range(batch.num_rows):\n        # Preprocess image\n        im = batch[view][x].as_py(uri_prefix).as_cv2()\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\n        # Inference\n        with torch.no_grad():\n            predictor = SamPredictor(self.model)\n            predictor.set_image(im)\n            img_embedding = predictor.get_image_embedding().cpu().numpy()\n\n        # Process model outputs\n        embeddings.append(img_embedding)\n    return embeddings\n</code></pre>"},{"location":"code/segment_anything/segment_anything/#segment_anything.segment_anything.SAM.export_to_onnx","title":"<code>export_to_onnx(library_dir)</code>","text":"<p>Export Torch model to ONNX</p> <p>Parameters:</p> Name Type Description Default <code>library_dir</code> <code>Path</code> <p>Dataset library directory</p> required Source code in <code>pixano_inference/segment_anything/segment_anything.py</code> <pre><code>def export_to_onnx(self, library_dir: Path):\n\"\"\"Export Torch model to ONNX\n\n    Args:\n        library_dir (Path): Dataset library directory\n    \"\"\"\n\n    # Model directory\n    model_dir = library_dir / \"models\"\n    model_dir.mkdir(parents=True, exist_ok=True)\n\n    # Put model to CPU for export\n    self.model.to(\"cpu\")\n\n    # Export settings\n    onnx_model = SamOnnxModel(self.model, return_single_mask=True)\n    dynamic_axes = {\n        \"point_coords\": {1: \"num_points\"},\n        \"point_labels\": {1: \"num_points\"},\n    }\n    embed_dim = self.model.prompt_encoder.embed_dim\n    embed_size = self.model.prompt_encoder.image_embedding_size\n    mask_input_size = [4 * x for x in embed_size]\n    dummy_inputs = {\n        \"image_embeddings\": torch.randn(\n            1, embed_dim, *embed_size, dtype=torch.float\n        ),\n        \"point_coords\": torch.randint(\n            low=0, high=1024, size=(1, 5, 2), dtype=torch.float\n        ),\n        \"point_labels\": torch.randint(\n            low=0, high=4, size=(1, 5), dtype=torch.float\n        ),\n        \"mask_input\": torch.randn(1, 1, *mask_input_size, dtype=torch.float),\n        \"has_mask_input\": torch.tensor([1], dtype=torch.float),\n        \"orig_im_size\": torch.tensor([1500, 2250], dtype=torch.float),\n    }\n    output_names = [\"masks\", \"iou_predictions\", \"low_res_masks\"]\n    onnx_path = model_dir / self.checkpoint_path.name.replace(\".pth\", \".onnx\")\n\n    # Export model\n    with warnings.catch_warnings():\n        warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n        warnings.filterwarnings(\"ignore\", category=UserWarning)\n        with open(onnx_path, \"wb\") as f:\n            torch.onnx.export(\n                onnx_model,\n                tuple(dummy_inputs.values()),\n                f,\n                export_params=True,\n                verbose=False,\n                opset_version=17,\n                do_constant_folding=True,\n                input_names=list(dummy_inputs.keys()),\n                output_names=output_names,\n                dynamic_axes=dynamic_axes,\n            )\n    # Quantize model\n    quantize_dynamic(\n        model_input=onnx_path,\n        model_output=onnx_path,\n        optimize_model=True,\n        per_channel=False,\n        reduce_range=False,\n        weight_type=QuantType.QUInt8,\n    )\n\n    # Put model back to device after export\n    self.model.to(self.device)\n</code></pre>"},{"location":"code/segment_anything/segment_anything/#segment_anything.segment_anything.SAM.inference_batch","title":"<code>inference_batch(batch, view, uri_prefix, threshold=0.0)</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[list[arrow_types.ObjectAnnotation]]</code> <p>list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation</p> Source code in <code>pixano_inference/segment_anything/segment_anything.py</code> <pre><code>def inference_batch(\n    self, batch: pa.RecordBatch, view: str, uri_prefix: str, threshold: float = 0.0\n) -&gt; list[list[arrow_types.ObjectAnnotation]]:\n\"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n\n    Returns:\n        list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation\n    \"\"\"\n\n    objects = []\n\n    # Iterate manually\n    for x in range(batch.num_rows):\n        # Preprocess image\n        im = batch[view][x].as_py(uri_prefix).as_cv2()\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n\n        # Inference\n        with torch.no_grad():\n            generator = SamAutomaticMaskGenerator(self.model)\n            output = generator.generate(im)\n\n        # Process model outputs\n        h, w = im.shape[:2]\n        objects.append(\n            [\n                arrow_types.ObjectAnnotation(\n                    id=shortuuid.uuid(),\n                    view_id=view,\n                    bbox=normalize(output[i][\"bbox\"], h, w),\n                    bbox_confidence=float(output[i][\"predicted_iou\"]),\n                    bbox_source=self.id,\n                    mask=mask_to_rle(output[i][\"segmentation\"]),\n                    mask_source=self.id,\n                    category_id=0,\n                    category_name=\"N/A\",\n                )\n                for i in range(len(output))\n                if output[i][\"predicted_iou\"] &gt; threshold\n            ]\n        )\n    return objects\n</code></pre>"},{"location":"code/tensorflow_models/efficientdet/","title":"EfficientDet","text":""},{"location":"code/tensorflow_models/efficientdet/#tensorflow_models.EfficientDet","title":"<code>tensorflow_models.EfficientDet(id='', device='/GPU:0')</code>","text":"<p>         Bases: <code>InferenceModel</code></p> <p>TensorFlow Hub EfficientDet Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>source</code> <code>str</code> <p>Model source</p> <code>info</code> <code>str</code> <p>Additional model info</p> <code>model</code> <code>tf.keras.Model</code> <p>TensorFlow model</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"/GPU:0\", \"/CPU:0\"). Defaults to \"/GPU:0\".</p> <code>'/GPU:0'</code> Source code in <code>pixano_inference/tensorflow_models/efficientdet.py</code> <pre><code>def __init__(self, id: str = \"\", device: str = \"/GPU:0\") -&gt; None:\n\"\"\"Initialize model\n\n    Args:\n        id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"/GPU:0\", \"/CPU:0\"). Defaults to \"/GPU:0\".\n    \"\"\"\n\n    super().__init__(\n        name=\"EfficientDet_D1\",\n        id=id,\n        device=device,\n        source=\"TensorFlow Hub\",\n        info=\"EfficientDet model, with D1 architecture\",\n    )\n\n    # Model\n    with tf.device(self.device):\n        self.model = hub.load(\"https://tfhub.dev/tensorflow/efficientdet/d1/1\")\n</code></pre>"},{"location":"code/tensorflow_models/efficientdet/#tensorflow_models.efficientdet.EfficientDet.inference_batch","title":"<code>inference_batch(batch, view, uri_prefix, threshold=0.0)</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[list[arrow_types.ObjectAnnotation]]</code> <p>list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation</p> Source code in <code>pixano_inference/tensorflow_models/efficientdet.py</code> <pre><code>def inference_batch(\n    self, batch: pa.RecordBatch, view: str, uri_prefix: str, threshold: float = 0.0\n) -&gt; list[list[arrow_types.ObjectAnnotation]]:\n\"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n\n    Returns:\n        list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation\n    \"\"\"\n\n    objects = []\n\n    # TF.Hub Models don't support image batches, so iterate manually\n    for x in range(batch.num_rows):\n        # Preprocess image\n        im = batch[view][x].as_py(uri_prefix).as_pillow()\n        im_tensor = tf.expand_dims(tf.keras.utils.img_to_array(im), 0)\n        im_tensor = tf.image.convert_image_dtype(im_tensor, dtype=\"uint8\")\n\n        # Inference\n        output = self.model(im_tensor)\n\n        # Process model outputs\n        objects.append(\n            [\n                arrow_types.ObjectAnnotation(\n                    id=shortuuid.uuid(),\n                    view_id=view,\n                    bbox=xyxy_to_xywh(\n                        [\n                            output[\"detection_boxes\"][0][i][1],\n                            output[\"detection_boxes\"][0][i][0],\n                            output[\"detection_boxes\"][0][i][3],\n                            output[\"detection_boxes\"][0][i][2],\n                        ]\n                    ),\n                    bbox_confidence=float(output[\"detection_scores\"][0][i]),\n                    bbox_source=self.id,\n                    category_id=int(output[\"detection_classes\"][0][i]),\n                    category_name=coco_names_91(output[\"detection_classes\"][0][i]),\n                )\n                for i in range(int(output[\"num_detections\"]))\n                if output[\"detection_scores\"][0][i] &gt; threshold\n            ]\n        )\n    return objects\n</code></pre>"},{"location":"code/tensorflow_models/fasterrcnn/","title":"FasterRCNN","text":""},{"location":"code/tensorflow_models/fasterrcnn/#tensorflow_models.FasterRCNN","title":"<code>tensorflow_models.FasterRCNN(id='', device='/GPU:0')</code>","text":"<p>         Bases: <code>InferenceModel</code></p> <p>TensorFlow Hub FasterRCNN Model</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Model name</p> <code>id</code> <code>str</code> <p>Model ID</p> <code>device</code> <code>str</code> <p>Model GPU or CPU device</p> <code>source</code> <code>str</code> <p>Model source</p> <code>info</code> <code>str</code> <p>Additional model info</p> <code>model</code> <code>tf.keras.Model</code> <p>TensorFlow model</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Previously used ID, generate new ID if \"\". Defaults to \"\".</p> <code>''</code> <code>device</code> <code>str</code> <p>Model GPU or CPU device (e.g. \"/GPU:0\", \"/CPU:0\"). Defaults to \"/GPU:0\".</p> <code>'/GPU:0'</code> Source code in <code>pixano_inference/tensorflow_models/fasterrcnn.py</code> <pre><code>def __init__(self, id: str = \"\", device: str = \"/GPU:0\") -&gt; None:\n\"\"\"Initialize model\n\n    Args:\n        id (str, optional): Previously used ID, generate new ID if \"\". Defaults to \"\".\n        device (str, optional): Model GPU or CPU device (e.g. \"/GPU:0\", \"/CPU:0\"). Defaults to \"/GPU:0\".\n    \"\"\"\n\n    super().__init__(\n        name=\"FasterRCNN_R50\",\n        id=id,\n        device=device,\n        source=\"TensorFlow Hub\",\n        info=\"FasterRCNN model, with ResNet50 architecture\",\n    )\n\n    # Model\n    with tf.device(self.device):\n        self.model = hub.load(\n            \"https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1\"\n        )\n</code></pre>"},{"location":"code/tensorflow_models/fasterrcnn/#tensorflow_models.fasterrcnn.FasterRCNN.inference_batch","title":"<code>inference_batch(batch, view, uri_prefix, threshold=0.0)</code>","text":"<p>Inference pre-annotation for a batch</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>pa.RecordBatch</code> <p>Input batch</p> required <code>view</code> <code>str</code> <p>Dataset view</p> required <code>uri_prefix</code> <code>str</code> <p>URI prefix for media files</p> required <code>threshold</code> <code>float</code> <p>Confidence threshold. Defaults to 0.0.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[list[arrow_types.ObjectAnnotation]]</code> <p>list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation</p> Source code in <code>pixano_inference/tensorflow_models/fasterrcnn.py</code> <pre><code>def inference_batch(\n    self, batch: pa.RecordBatch, view: str, uri_prefix: str, threshold: float = 0.0\n) -&gt; list[list[arrow_types.ObjectAnnotation]]:\n\"\"\"Inference pre-annotation for a batch\n\n    Args:\n        batch (pa.RecordBatch): Input batch\n        view (str): Dataset view\n        uri_prefix (str): URI prefix for media files\n        threshold (float, optional): Confidence threshold. Defaults to 0.0.\n\n    Returns:\n        list[list[arrow_types.ObjectAnnotation]]: Model inferences as lists of ObjectAnnotation\n    \"\"\"\n\n    objects = []\n\n    # TF.Hub Models don't support image batches, so iterate manually\n    for x in range(batch.num_rows):\n        # Preprocess image\n        im = batch[view][x].as_py(uri_prefix).as_pillow()\n        im_tensor = tf.expand_dims(tf.keras.utils.img_to_array(im), 0)\n        im_tensor = tf.image.convert_image_dtype(im_tensor, dtype=\"uint8\")\n\n        # Inference\n        output = self.model(im_tensor)\n\n        # Process model outputs\n        objects.append(\n            [\n                arrow_types.ObjectAnnotation(\n                    id=shortuuid.uuid(),\n                    view_id=view,\n                    bbox=xyxy_to_xywh(\n                        [\n                            output[\"detection_boxes\"][0][i][1],\n                            output[\"detection_boxes\"][0][i][0],\n                            output[\"detection_boxes\"][0][i][3],\n                            output[\"detection_boxes\"][0][i][2],\n                        ]\n                    ),\n                    bbox_confidence=float(output[\"detection_scores\"][0][i]),\n                    bbox_source=self.id,\n                    category_id=int(output[\"detection_classes\"][0][i]),\n                    category_name=coco_names_91(output[\"detection_classes\"][0][i]),\n                )\n                for i in range(int(output[\"num_detections\"]))\n                if output[\"detection_scores\"][0][i] &gt; threshold\n            ]\n        )\n    return objects\n</code></pre>"},{"location":"user/","title":"Getting started with Pixano Inference","text":"<ul> <li>Get started with Pixano</li> <li>Install Pixano Inference</li> <li>Use Pixano Inference<ul> <li>Check out this Jupyter notebook for using the Pixano Inference models for pre-annotation</li> <li>Check out this Jupyter notebook for using the Pixano Inference models for interactive annotation</li> </ul> </li> </ul>"},{"location":"user/install/","title":"Installing Pixano Inference","text":"<p>As Pixano and Pixano Inference require specific versions for their dependencies, we recommend creating a new Python virtual environment to install them.</p> <p>For example, with conda:</p> <pre><code>conda create -n pixano_env python=3.10\nconda activate pixano_env\n</code></pre> <p>Then, you can install the Pixano and Pixano Inference packages inside that environment with pip:</p> <pre><code>pip install pixano\npip install pixano-inference@git+https://github.com/pixano/pixano-inference\n</code></pre>"},{"location":"user/interactive_annotation/","title":"Interactive annotation","text":"<p>Please refer to this Jupyter notebook for information on how to use inference models for interactive annotation.</p>"},{"location":"user/pre_annotation/","title":"Pre-annotation","text":"<p>Please refer to this Jupyter notebook for information on how to use inference models for pre-annotation.</p>"}]}